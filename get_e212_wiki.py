from bs4 import BeautifulSoup
from collections import defaultdict
import requests
from pprint import pformat
from pathlib import Path
import os
import typing
import time
from datetime import timedelta
import tempfile
import string
import re


def get_wiki_page(url: str) -> Path:
    """Download a page.

    Stores the downloaded page in the OS temp dir, for up to 5 days, to allow
    for faster iteration when improving the script.
    """
    tmp_dir = Path(tempfile.gettempdir())
    outfile = tmp_dir / Path(os.path.basename(url))
    now = time.time()
    max_age = timedelta(days=5) / timedelta(seconds=1)
    if outfile.exists() and outfile.stat().st_mtime > now - max_age:
        print(f"{outfile} exists and is recent")
        return outfile

    print(f"downloading {url} to {outfile}")
    with open(outfile, "bw+") as fout:
        page_resp = requests.get(url)
        fout.write(page_resp.content)

    return outfile


OPERATOR_NAME_CLEANER = re.compile(r"\s+(\[.+\])?$")
CONTRY_HEADER = re.compile(r"^.+(?P<country_code>[A-Z]{2}(-[A-Z]{2})?)")
SKIPS = [
    ("IL", "425", "05"),  # Israeli PLMN used by a Palestinian operator
    ("IL", "425", "06"),  # Israeli PLMN used by a Palestinian operator
]

# Some countries share MCC, and since the dataset groups by MCC, it can make
# it difficult to figure out the correct country for those MCCs.
# This provides a small override table to correct for those cases.
OVERRIDES = {
    ("340", "01"): "GP",
}


def extract_page(path):
    """Yield MCC, MNC, country code and operator name entries from a page.

    Scrapes the wikipedia page that mostly follows a rigid structure.
    """
    with open(path, encoding="utf8") as fin:
        soup = BeautifulSoup(fin.read(), features="lxml")

    for country_header in soup.find_all("h4"):
        match = CONTRY_HEADER.match(country_header.text.strip())
        if match is None:
            raise ValueError("No country match")
        country_code = match["country_code"]

        table_rows = country_header.find_next_sibling("table").find_all("tr")
        for row in table_rows:
            columns = row.find_all("td")
            if not columns:
                continue

            mcc = columns[0].text.strip()
            if not all(digit in string.digits for digit in mcc):
                continue

            if columns[4].text == "Not operational":
                continue

            mnc = columns[1].text.strip()
            if not all(digit in string.digits for digit in mnc):
                continue

            if (country_code, mcc, mnc) in SKIPS:
                continue

            operator_name = OPERATOR_NAME_CLEANER.sub("", columns[3].text)

            if (mcc, mnc) in OVERRIDES:
                yield mcc, mnc, OVERRIDES[(mcc, mnc)], operator_name

            else:
                yield mcc, mnc, country_code, operator_name


def main() -> None:
    """Extract information from a bunch of wiki pages to build the datasets from.

    While wiki is imperfect, the mappings between the datasets are not perfect
    either as some MCCs belong to government entities that are disputed, and some
    MCCs are shared by multiple countries.
    This makes a simple general mapping impossible and the code has to be able
    to handle a bunch of exceptions and just try to do a best effort mapping.
    """
    files = [
        "https://en.wikipedia.org/wiki/Mobile_network_codes_in_ITU_region_2xx_(Europe)",
        "https://en.wikipedia.org/wiki/Mobile_Network_Codes_in_ITU_region_3xx_(North_America)",
        "https://en.wikipedia.org/wiki/Mobile_network_codes_in_ITU_region_4xx_(Asia)",
        "https://en.wikipedia.org/wiki/Mobile_Network_Codes_in_ITU_region_5xx_(Oceania)",
        "https://en.wikipedia.org/wiki/Mobile_Network_Codes_in_ITU_region_6xx_(Africa)",
        "https://en.wikipedia.org/wiki/Mobile_Network_Codes_in_ITU_region_7xx_(South_America)",
    ]

    paths = [get_wiki_page(filename) for filename in files]

    extracted_pages = (values for path in paths for values in extract_page(path))

    operator_by_plmn = defaultdict(dict)
    for mcc, mnc, country_code, operator_name in extracted_pages:
        operator_by_plmn[int(mcc)][int(mnc)] = (country_code, operator_name)
    operator_by_plmn = dict(operator_by_plmn)

    countriesdict = defaultdict(list)
    for mcc, mncs in operator_by_plmn.items():
        for mnc, country in mncs.items():
            countriesdict[country[0]].append((mcc, mnc))
    countriesdict = dict(countriesdict)

    with open("phone_iso3166/e212_names.py", "w", encoding="utf8") as fout:
        fout.write("# pylint: disable=too-many-lines\n")
        fout.write("#!/usr/bin/env python\n")
        fout.write("# -*- coding: utf-8 -*-\n")
        fout.write("# Generated by get_e212.py\n")
        fout.write("# Based on https://en.wikipedia.org/wiki/Mobile_country_code\n")
        fout.write("operators = \\\n")
        fout.write(pformat(operator_by_plmn))

        fout.write("\n\n\ncountries = \\\n")
        fout.write(pformat(countriesdict))
        fout.write("\n")

    def reduce_operators(
        operators: typing.Dict[int, typing.Tuple[str, str]],
    ) -> typing.Union[str, typing.Dict[int, str]]:

        countries = {each[0] for each in operators.values()}
        if len(countries) == 1:
            return countries.pop()

        return {mnc: operator[0] for mnc, operator in operators.items()}

    plmns_to_country = {}
    for mcc, operator in operator_by_plmn.items():
        plmns_to_country[mcc] = reduce_operators(operator)

    with open("phone_iso3166/e212.py", "w", encoding="utf8") as fout:
        fout.write("# Generated by get_e212.py\n")
        fout.write("networks = ")
        fout.write(pformat(plmns_to_country))
        fout.write("\n")


if __name__ == "__main__":
    main()
